{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement  \n",
    "\n",
    "The goal is to forecast the demand for bikes in dependency of weather conditions like outside temperature and calendric informations e.g. holidays. These information and the demand structure is provided in a set with two years of daily historic data.  \n",
    "The demand is given as the total daily demand and as a split for registered users and casual users. To increase the quality of the prediction registered user demand and casual user demand will be predicted separately in step two.  \n",
    "To make predictions machine learning is used to train regressors. Scikit-Learn recommends a support vector regressor (SVR) for this kind of problem and data amount. In addition a deep neuronal network (DNN) regressor is trained for comparison. To find the hyper-parameters for these regressors grid search and randomized search are utilized. Due to the small dataset cross validation is applied.    \n",
    "\n",
    "> http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR  \n",
    "> https://github.com/tensorflow/skflow/blob/master/g3doc/api_docs/python/estimators.md  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Fetching Dataset\n",
    "\n",
    "bike_data = pd.read_csv(\"day.csv\", header=0)\n",
    "\n",
    "print(\"Data read successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>331</td>\n",
       "      <td>654</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>131</td>\n",
       "      <td>670</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>120</td>\n",
       "      <td>1229</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>108</td>\n",
       "      <td>1454</td>\n",
       "      <td>1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>82</td>\n",
       "      <td>1518</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1        0        6           0   \n",
       "1        2  2011-01-02       1   0     1        0        0           0   \n",
       "2        3  2011-01-03       1   0     1        0        1           1   \n",
       "3        4  2011-01-04       1   0     1        0        2           1   \n",
       "4        5  2011-01-05       1   0     1        0        3           1   \n",
       "\n",
       "   weathersit      temp     atemp       hum  windspeed  casual  registered  \\\n",
       "0           2  0.344167  0.363625  0.805833   0.160446     331         654   \n",
       "1           2  0.363478  0.353739  0.696087   0.248539     131         670   \n",
       "2           1  0.196364  0.189405  0.437273   0.248309     120        1229   \n",
       "3           1  0.200000  0.212122  0.590435   0.160296     108        1454   \n",
       "4           1  0.226957  0.229270  0.436957   0.186900      82        1518   \n",
       "\n",
       "    cnt  \n",
       "0   985  \n",
       "1   801  \n",
       "2  1349  \n",
       "3  1562  \n",
       "4  1600  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column(s):\n",
      "Index(['instant', 'dteday', 'season', 'yr', 'mnth', 'holiday', 'weekday',\n",
      "       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed'],\n",
      "      dtype='object')\n",
      "\n",
      "Target column:\n",
      "cnt\n"
     ]
    }
   ],
   "source": [
    "# Extracting\n",
    "\n",
    "feature_cols = bike_data.columns[:-3]  # all columns but last are features\n",
    "target_col = bike_data.columns[-1]  # last column is the target\n",
    "\n",
    "print (\"Feature column(s):\\n{}\\n\".format(feature_cols))\n",
    "print (\"Target column:\\n{}\".format(target_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Calculate Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profit(y,y_cap, revenue=3,loanCost=2):\n",
    "    return revenue * np.minimum(y[::1], y_cap[::1]) - loanCost * y_cap[::1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Calculate Cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost (y_cap, loanCost):\n",
    "    return loanCost * y_cap[::1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Calculate Profit as percentage of total expenditure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profit_percentage(y,y_cap, revenue, loanCost):\n",
    "    return ((revenue * np.minimum(y[::1], y_cap[::1]) - loanCost * y_cap[::1]).sum() / (loanCost * y_cap[::1]).sum() ) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Convert from percentage to Actual Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToPrediction(data,percentage_predictions):\n",
    "    demand = np.around(data + (np.multiply(data, percentage_predictions)/100))\n",
    "    demand[demand <0 ] = 0\n",
    "    return demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RMSLE Scorer\n",
    "One common way to evaluate regression model is through calculating MSE or RMSE. In this particular project , the metric to evaluate our model is Root Mean Square Logarithmic Error (RMSLE). RMSLE is particularly helpful when you want to penalize an under-predicted estimate greater than an over-predicted estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y, y_):\n",
    "    log1 = np.nan_to_num(np.array([np.log(v + 1) for v in y]))\n",
    "    log2 = np.nan_to_num(np.array([np.log(v + 1) for v in y_]))\n",
    "    calc = (log1 - log2) ** 2\n",
    "    return np.sqrt(np.mean(calc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the base model the demand for today is the previous days demand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184,)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actual = bike_data[target_col][547:731]  # corresponding targets\n",
    "y_actual = y_actual.reset_index(drop = True)\n",
    "y_actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_staged = y_actual.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "data.insert(0, bike_data[target_col][546])\n",
    "data.insert(0, bike_data[target_col][545])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_df = pd.concat([pd.DataFrame(data), y_staged], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_df.drop(y_predicted_df.tail(2).index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = y_predicted_df[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate Base Model Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "805278\n"
     ]
    }
   ],
   "source": [
    "print(profit(y_actual,y_predicted).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost is:2237052\n",
      "profit is:805278\n",
      "profit percentage is:35.9972857135194\n"
     ]
    }
   ],
   "source": [
    "print(\"cost is:\" + str(cost(y_predicted,2).sum()) )\n",
    "print(\"profit is:\" + str(profit(y_actual,y_predicted).sum()))\n",
    "print(\"profit percentage is:\" + str(profit_percentage(y_actual,y_predicted, 3, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Alternate Cost Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost is:2237052\n",
      "profit is:-6009.999999999811\n",
      "profit percentage is:-0.26865714341909847\n"
     ]
    }
   ],
   "source": [
    "print(\"cost is:\" + str(cost(y_predicted,2).sum()) )\n",
    "print(\"profit is:\" + str(profit(y_actual,y_predicted,revenue = 2.2,loanCost=2 ).sum()))\n",
    "print(\"profit percentage is:\" + str(profit_percentage(y_actual,y_predicted, 2.2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset with percentage change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"processed_Data.csv\", header=0)\n",
    "data['instant'] = data['instant'] % 30\n",
    "X_raw_train = data[0:541] # Training Test contains 18 months training data - till 30th June 2012\n",
    "X_raw_test  = data[541:] # Test set contains 6 months test data - from 1st of July 2012 to 31st December 2012\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =[\n",
    "       \"season__1\",\"season__2\",\"season__3\",\"season__4\",\"season__5\",\n",
    "       \"weathersit__1\",\"weathersit__2\",\"weathersit__3\",\"weathersit__4\",\"weathersit__5\",\n",
    "        \"cnt__1\",\n",
    "        \"atemp\",\"hum\",\"windspeed\",\n",
    "        \"mnth\",\"instant\",\"holiday\",\"weekday\",\"workingday\",\n",
    "        \"moving_avg_weekly_cnt\"]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_raw_train[cols].values.tolist()\n",
    "y_train_df = X_raw_train[['demand_pc_inc']]\n",
    "y_train = y_train_df['demand_pc_inc'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_raw_test[cols].values.tolist()\n",
    "y_test_df = X_raw_test[['demand_pc_inc']]\n",
    "y_test = y_test_df['demand_pc_inc'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cnt = data['cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_predictions = data_cnt[541:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_for_calculations = data_cnt[539:723].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensemble Methods\n",
    "\n",
    "Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking). Ensemble methods can be divided into two groups: sequential ensemble methods where the base learners are generated sequentially (e.g. AdaBoost) and parallel ensemble methods where the base learners are generated in parallel (e.g. Random Forest). The basic motivation of sequential methods is to exploit the dependence between the base learners since the overall performance can be boosted by weighing previously mislabeled examples with higher weight. The basic motivation of parallel methods is to exploit independence between the base learners since the error can be reduced dramatically by averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting\n",
    "Boosting refers to a family of algorithms that are able to convert weak learners to strong learners. The main principle of boosting is to fit a sequence of weak learners (models that are only slightly better than random guessing, such as small decision trees) to weighted versions of the data, where more weight is given to examples that were mis-classified by earlier rounds. The predictions are then combined through a weighted majority vote (classification) or a weighted sum (regression) to produce the final prediction. The principal difference between boosting and the committee methods such as bagging is that base learners are trained in sequence on a weighted version of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Adaptive Boosting\n",
    "\n",
    "The algorithm below describes the most widely used form of boosting algorithm called AdaBoost, which stands for adaptive boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "805749.0\n"
     ]
    }
   ],
   "source": [
    "### ADA Boost Regressor\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import pipeline,metrics,grid_search\n",
    "\n",
    "#regressor = RandomForestRegressor(random_state = 0, max_depth = 30, n_estimators = 500, max_features = 'log2')\n",
    "regressor = AdaBoostRegressor()\n",
    "estimator_ada = pipeline.Pipeline(steps = [       \n",
    "    ('model_fitting', regressor)\n",
    "    ]\n",
    ")\n",
    "estimator_ada.fit(X_train, y_train)\n",
    "pred_ada = estimator_ada.predict(X_test)\n",
    "model_predictions_ada = convertToPrediction(y_for_calculations,pred_ada)\n",
    "print(profit(actual_predictions,model_predictions_ada).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "789402.0\n"
     ]
    }
   ],
   "source": [
    "num_est = [1, 2, 3, 10,50,100,200,300,500]\n",
    "learning_rate = [0.01,0.1,0.5,1]\n",
    "loss = ['linear', 'square', 'exponential']\n",
    "params_dict={'n_estimators':num_est,'learning_rate': learning_rate,'loss':loss}\n",
    "clf_ada=GridSearchCV(estimator=AdaBoostRegressor(),param_grid=params_dict,scoring='neg_mean_squared_error')\n",
    "clf_ada.fit(X_train,y_train)\n",
    "pred_ada=clf_ada.predict(X_test)\n",
    "model_predictions_ada = convertToPrediction(y_for_calculations,pred_ada)\n",
    "print(profit(actual_predictions,model_predictions_ada).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'learning_rate': 0.5, 'loss': 'linear', 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: \", clf_ada.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "824656.0\n"
     ]
    }
   ],
   "source": [
    "regressor = AdaBoostRegressor(n_estimators = 200, loss = 'square')\n",
    "estimator_ada = pipeline.Pipeline(steps = [       \n",
    "    ('model_fitting', regressor)\n",
    "    ]\n",
    ")\n",
    "estimator_ada.fit(X_train, y_train)\n",
    "pred_ada = estimator_ada.predict(X_test)\n",
    "model_predictions_ada = convertToPrediction(y_for_calculations,pred_ada)\n",
    "print(profit(actual_predictions,model_predictions_ada).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE Value:  0.5747857197360348\n"
     ]
    }
   ],
   "source": [
    "print (\"RMSLE Value: \",rmsle(actual_predictions,model_predictions_ada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost is:2215910.0\n",
      "profit is:824656.0\n",
      "profit percentage is:37.21522986041852\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "print(\"cost is:\" + str(cost(model_predictions_ada,2).sum()) )\n",
    "print(\"profit is:\" + str(profit(actual_predictions,model_predictions_ada).sum()))\n",
    "print(\"profit percentage is:\" + str(profit_percentage(actual_predictions,model_predictions_ada, 3, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation metrics\n",
    "from sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  # for classification\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor\n",
      "877396.0\n",
      "cost is:2108060.0\n",
      "profit is:877396.0\n",
      "profit percentage is:41.621016479606844\n",
      "GradientBoostingRegressor\n",
      "923503.0\n",
      "cost is:2090438.0\n",
      "profit is:923503.0\n",
      "profit percentage is:44.1774881627678\n",
      "AdaBoostRegressor\n",
      "750867.0\n",
      "cost is:2406612.0\n",
      "profit is:750867.0\n",
      "profit percentage is:31.200168535684192\n",
      "BaggingRegressor\n",
      "871950.0\n",
      "cost is:2073552.0\n",
      "profit is:871950.0\n",
      "profit percentage is:42.05103127387208\n",
      "SVR\n",
      "800346.0\n",
      "cost is:2254326.0\n",
      "profit is:800346.0\n",
      "profit percentage is:35.50267352636664\n",
      "KNeighborsRegressor\n",
      "817328.0\n",
      "cost is:2158960.0\n",
      "profit is:817328.0\n",
      "profit percentage is:37.85748693815541\n"
     ]
    }
   ],
   "source": [
    "models=[RandomForestRegressor(),GradientBoostingRegressor(),AdaBoostRegressor(),BaggingRegressor(),SVR(),KNeighborsRegressor()]\n",
    "model_names=['RandomForestRegressor','GradientBoostingRegressor','AdaBoostRegressor','BaggingRegressor','SVR','KNeighborsRegressor']\n",
    "rmsle=[]\n",
    "d={}\n",
    "for model in range (len(models)):\n",
    "    clf=models[model]\n",
    "    print(model_names[model])\n",
    "    clf.fit(X_train,y_train)\n",
    "    test_pred=clf.predict(X_test)\n",
    "    model_predictions = convertToPrediction(y_for_calculations,test_pred)\n",
    "    print(profit(actual_predictions,model_predictions).sum())\n",
    "    print(\"cost is:\" + str(cost(model_predictions,2).sum()) )\n",
    "    print(\"profit is:\" + str(profit(actual_predictions,model_predictions).sum()))\n",
    "    print(\"profit percentage is:\" + str(profit_percentage(actual_predictions,model_predictions, 3, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW LET'S Dig deeper into each of these ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "The regressors are trained using randomized search and cross-validation to identify the area of the best parameters. Then a grid search is used to tune parameter values of the regressor functions.\n",
    "\n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html  \n",
    "> http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "865691.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for random forest regresion.\n",
    "no_of_test=[500]\n",
    "params_dict={'n_estimators':no_of_test,'n_jobs':[-1],'max_features':[\"auto\",'sqrt','log2'],'max_depth':[10,20,30]}\n",
    "clf_rf=GridSearchCV(estimator=RandomForestRegressor(),param_grid=params_dict,scoring='neg_mean_squared_error')\n",
    "clf_rf.fit(X_train,y_train)\n",
    "pred_rf=clf_rf.predict(X_test)\n",
    "model_predictions_rf = convertToPrediction(y_for_calculations,pred_rf)\n",
    "print(profit(actual_predictions,model_predictions_rf).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'max_depth': 20, 'max_features': 'log2', 'n_estimators': 500, 'n_jobs': -1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: \", clf_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "867925.0\n",
      "cost is:2090348.0\n",
      "profit is:867925.0\n",
      "profit percentage is:41.52059848408016\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import pipeline,metrics,grid_search\n",
    "\n",
    "#regressor = RandomForestRegressor(random_state = 0, max_depth = 30, n_estimators = 500, max_features = 'log2')\n",
    "regressor = RandomForestRegressor()\n",
    "estimator_rf = pipeline.Pipeline(steps = [       \n",
    "    ('model_fitting', regressor)\n",
    "    ]\n",
    ")\n",
    "estimator_rf.fit(X_train, y_train)\n",
    "pred_rf = estimator_rf.predict(X_test)\n",
    "model_predictions_rf = convertToPrediction(y_for_calculations,pred_rf)\n",
    "print(profit(actual_predictions,model_predictions_rf).sum())\n",
    "print(\"cost is:\" + str(cost(model_predictions_rf,2).sum()) )\n",
    "print(\"profit is:\" + str(profit(actual_predictions,model_predictions_rf).sum()))\n",
    "print(\"profit percentage is:\" + str(profit_percentage(actual_predictions,model_predictions_rf, 3, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "818132.0\n"
     ]
    }
   ],
   "source": [
    "#for Gradient Boosting regresion.\n",
    "no_of_estimators=[100,200,300,400,500]\n",
    "params_dict={'n_estimators':no_of_estimators,'learning_rate':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "             'max_features':[\"auto\",'sqrt','log2'],'max_depth':[10,20,30,40,50]}\n",
    "clf_gbr=GridSearchCV(estimator=GradientBoostingRegressor(),param_grid=params_dict,scoring='neg_mean_squared_error')\n",
    "clf_gbr.fit(X_train,y_train)\n",
    "pred=clf_gbr.predict(X_test)\n",
    "model_predictions = convertToPrediction(y_for_calculations,pred)\n",
    "print(profit(actual_predictions,model_predictions).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'learning_rate': 0.4, 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: \", clf_gbr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "922707.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#gbr = GradientBoostingRegressor(n_estimators=500,max_features= 'log2', learning_rate=0.1, max_depth = 10)\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "estimator_gbr = pipeline.Pipeline(steps = [       \n",
    "    ('model_fitting', gbr)\n",
    "    ]\n",
    ")\n",
    "estimator_gbr.fit(X_train, y_train)\n",
    "pred_gbr = estimator_gbr.predict(X_test)\n",
    "model_predictions_gbr = convertToPrediction(y_for_calculations,pred_gbr)\n",
    "print(profit(actual_predictions,model_predictions_gbr).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost is:2087850.0\n",
      "profit is:922707.0\n",
      "profit percentage is:44.194123141030246\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "print(\"cost is:\" + str(cost(model_predictions_gbr,2).sum()) )\n",
    "print(\"profit is:\" + str(profit(actual_predictions,model_predictions_gbr).sum()))\n",
    "print(\"profit percentage is:\" + str(profit_percentage(actual_predictions,model_predictions_gbr, 3, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADA Boost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780046.0\n"
     ]
    }
   ],
   "source": [
    "### ADA Boost Regressor\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import pipeline,metrics,grid_search\n",
    "\n",
    "#regressor = RandomForestRegressor(random_state = 0, max_depth = 30, n_estimators = 500, max_features = 'log2')\n",
    "regressor = AdaBoostRegressor()\n",
    "estimator_ada = pipeline.Pipeline(steps = [       \n",
    "    ('model_fitting', regressor)\n",
    "    ]\n",
    ")\n",
    "estimator_ada.fit(X_train, y_train)\n",
    "pred_ada = estimator_ada.predict(X_test)\n",
    "model_predictions_ada = convertToPrediction(y_for_calculations,pred_ada)\n",
    "print(profit(actual_predictions,model_predictions_ada).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885545.0\n",
      "cost is:2063812.0\n",
      "profit is:885545.0\n",
      "profit percentage is:42.90822032239371\n"
     ]
    }
   ],
   "source": [
    "### Bagging Regressor\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn import pipeline,metrics,grid_search\n",
    "\n",
    "#regressor = RandomForestRegressor(random_state = 0, max_depth = 30, n_estimators = 500, max_features = 'log2')\n",
    "regressor = BaggingRegressor()\n",
    "estimator_bagging = pipeline.Pipeline(steps = [       \n",
    "    ('model_fitting', regressor)\n",
    "    ]\n",
    ")\n",
    "estimator_bagging.fit(X_train, y_train)\n",
    "pred_bagging = estimator_bagging.predict(X_test)\n",
    "model_predictions_bagging = convertToPrediction(y_for_calculations,pred_bagging)\n",
    "print(profit(actual_predictions,model_predictions_bagging).sum())\n",
    "print(\"cost is:\" + str(cost(model_predictions_bagging,2).sum()) )\n",
    "print(\"profit is:\" + str(profit(actual_predictions,model_predictions_bagging).sum()))\n",
    "print(\"profit percentage is:\" + str(profit_percentage(actual_predictions,model_predictions_bagging, 3, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "817328.0\n"
     ]
    }
   ],
   "source": [
    "### KNN Regressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import pipeline,metrics,grid_search\n",
    "\n",
    "\n",
    "regressor = KNeighborsRegressor()\n",
    "estimator_knn = pipeline.Pipeline(steps = [       \n",
    "    ('model_fitting', regressor)\n",
    "    ]\n",
    ")\n",
    "estimator_knn.fit(X_train, y_train)\n",
    "pred_knn = estimator_knn.predict(X_test)\n",
    "model_predictions_knn = convertToPrediction(y_for_calculations,pred_knn)\n",
    "print(profit(actual_predictions,model_predictions_knn).sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training SVR\n",
    "svr = SVR()\n",
    "svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score SVR: -0.005914\n",
      "RMSE SVR: 1858.762813\n"
     ]
    }
   ],
   "source": [
    "# Validation SVR\n",
    "\n",
    "pred_svr = svr.predict(X_test)\n",
    "score_svr = r2_score(y_test, pred_svr)\n",
    "rmse_svr = sqrt(mean_squared_error(y_test, pred_svr))\n",
    "\n",
    "print(\"Score SVR: %f\" % score_svr)\n",
    "print(\"RMSE SVR: %f\" % rmse_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=SVR(C=1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
      "       fit_params={}, iid=True, n_jobs=-1,\n",
      "       param_grid=[{'C': [1000, 3000, 10000], 'kernel': ['linear', 'rbf']}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring='r2', verbose=0)\n",
      "\n",
      "Best parameter from grid search: {'C': 1000, 'kernel': 'rbf'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tuning SVR with GridSearch\n",
    "\n",
    "tuned_parameters = [{'C': [1000, 3000, 10000], \n",
    "                     'kernel': ['linear', 'rbf']}\n",
    "                   ]\n",
    "\n",
    "#svr_tuned = GridSearchCV(SVR (C=1), param_grid = tuned_parameters, scoring = 'mean_squared_error') #default 3-fold cross-validation, score method of the estimator\n",
    "svr_tuned_GS = GridSearchCV(SVR (C=1), param_grid = tuned_parameters, scoring = 'r2', n_jobs=-1) #default 3-fold cross-validation, score method of the estimator\n",
    "\n",
    "svr_tuned_GS.fit(X_train, y_train)\n",
    "\n",
    "print (svr_tuned_GS)\n",
    "print ('\\n' \"Best parameter from grid search: \" + str(svr_tuned_GS.best_params_) +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR Results\n",
      "\n",
      "Score SVR: -0.005914\n",
      "Score SVR tuned GS: -0.005236\n",
      "\n",
      "RMSE SVR: 1858.762813\n",
      "RMSE SVR tuned GS: 1858.135580\n"
     ]
    }
   ],
   "source": [
    "svr_tuned_pred_GS = svr_tuned_GS.predict(X_test)\n",
    "\n",
    "score_svr_tuned_GS = r2_score(y_test, svr_tuned_pred_GS)\n",
    "rmse_svr_tuned_GS = sqrt(mean_squared_error(y_test, svr_tuned_pred_GS))\n",
    "\n",
    "print(\"SVR Results\\n\")\n",
    "\n",
    "print(\"Score SVR: %f\" % score_svr)\n",
    "print(\"Score SVR tuned GS: %f\" % score_svr_tuned_GS)\n",
    "\n",
    "print(\"\\nRMSE SVR: %f\" % rmse_svr)\n",
    "print(\"RMSE SVR tuned GS: %f\" % rmse_svr_tuned_GS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "716309.0\n"
     ]
    }
   ],
   "source": [
    "svr_tuned_pred_GS\n",
    "\n",
    "\n",
    "##Profit Calculation for pct approach\n",
    "model_predictions = convertToPrediction(y_for_calculations,svr_tuned_pred_GS)\n",
    "print(profit(actual_predictions,model_predictions).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best CV score from grid search: -0.000350\n",
      "corresponding parameters: {'C': 8214.090468706976, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# SVR tuned with RandomizesSearch\n",
    "# may take a while!\n",
    "\n",
    "# Parameters\n",
    "param_dist = {  'C': sp_uniform (1000, 10000), \n",
    "                'kernel': ['rbf']\n",
    "             }\n",
    "\n",
    "n_iter_search = 1\n",
    "\n",
    "# MSE optimized\n",
    "#SVR_tuned_RS = RandomizedSearchCV(SVR (C=1), param_distributions = param_dist, scoring = 'mean_squared_error', n_iter=n_iter_search)\n",
    "\n",
    "# R^2 optimized\n",
    "SVR_tuned_RS = RandomizedSearchCV(SVR (C=1), param_distributions = param_dist, scoring = 'r2', n_iter=n_iter_search)\n",
    "\n",
    "# Fit\n",
    "SVR_tuned_RS.fit(X_train, y_train)\n",
    "\n",
    "# Best score and corresponding parameters.\n",
    "print('best CV score from grid search: {0:f}'.format(SVR_tuned_RS.best_score_))\n",
    "print('corresponding parameters: {}'.format(SVR_tuned_RS.best_params_))\n",
    "\n",
    "# Predict and score\n",
    "predict = SVR_tuned_RS.predict(X_test)\n",
    "\n",
    "score_svr_tuned_RS = r2_score(y_test, predict)\n",
    "rmse_svr_tuned_RS = sqrt(mean_squared_error(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR Results\n",
      "\n",
      "Score SVR: -0.005914\n",
      "Score SVR tuned GS: -0.005236\n",
      "Score SVR tuned RS: -0.005236\n",
      "\n",
      "RMSE SVR: 1858.762813\n",
      "RMSE SVR tuned GS: 1858.135580\n",
      "RMSE SVR tuned RS: 1858.135580\n"
     ]
    }
   ],
   "source": [
    "print('SVR Results\\n')\n",
    "\n",
    "print(\"Score SVR: %f\" % score_svr)\n",
    "print(\"Score SVR tuned GS: %f\" % score_svr_tuned_GS)\n",
    "print(\"Score SVR tuned RS: %f\" % score_svr_tuned_RS)\n",
    "\n",
    "print(\"\\nRMSE SVR: %f\" % rmse_svr)\n",
    "print(\"RMSE SVR tuned GS: %f\" % rmse_svr_tuned_GS)\n",
    "print(\"RMSE SVR tuned RS: %f\" % rmse_svr_tuned_RS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "716309.0\n"
     ]
    }
   ],
   "source": [
    "##Profit Calculation for pct approach\n",
    "model_predictions = convertToPrediction(y_for_calculations,predict)\n",
    "print(profit(actual_predictions,model_predictions).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from time import time\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_model = MLPRegressor(hidden_layer_sizes=(5,),\n",
    "                                       activation='relu',\n",
    "                                       solver='adam',\n",
    "                                       learning_rate='adaptive',\n",
    "                                       max_iter=15000,\n",
    "                                       learning_rate_init=0.01,\n",
    "                                       alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = int(time() * 1000)\n",
    "bike_model.fit(X_train, y_train)\n",
    "end_time = int(time() * 1000)\n",
    "logging.debug('Finished training universal model')\n",
    "logging.debug('Training took {} ms'.format(end_time - start_time)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dnn = bike_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions_dnn = convertToPrediction(y_for_calculations,pred_dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "788996.0\n"
     ]
    }
   ],
   "source": [
    "print(profit(actual_predictions,model_predictions_dnn).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averageing best predictions from different regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891018.0\n",
      "cost is:2154726.0\n",
      "profit is:891018.0\n",
      "profit percentage is:41.35180064657873\n"
     ]
    }
   ],
   "source": [
    "w = [1, 1, 1, 1]  # weights\n",
    "pred_agg = np.c_[pred_rf,pred_gbr,pred_ada,pred_bagging]\n",
    "pred_avr = np.average(pred_agg, axis=1, weights=w)\n",
    "model_predictions_avr = convertToPrediction(y_for_calculations,pred_avr)\n",
    "print(profit(actual_predictions,model_predictions_avr).sum())\n",
    "print(\"cost is:\" + str(cost(model_predictions_avr,2).sum()) )\n",
    "print(\"profit is:\" + str(profit(actual_predictions,model_predictions_avr).sum()))\n",
    "print(\"profit percentage is:\" + str(profit_percentage(actual_predictions,model_predictions_avr, 3, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble : Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_rf = estimator_rf.predict(X_train)\n",
    "pred_train_gbr = estimator_gbr.predict(X_train)\n",
    "pred_train_ada = estimator_ada.predict(X_train)\n",
    "pred_train_bagging = estimator_bagging.predict(X_train)\n",
    "pred_train_knn = estimator_knn.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = pd.DataFrame({'rf':pred_train_rf, 'gbr':pred_train_gbr, 'ada':pred_train_ada,\n",
    "                      'bagging':pred_train_bagging,'true':y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING:tensorflow:From <ipython-input-61-243d79292383>:5: infer_real_valued_columns_from_input (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please specify feature columns explicitly.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:142: setup_train_data_feeder (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:96: extract_dask_data (from tensorflow.contrib.learn.python.learn.learn_io.dask_io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please feed input to tf.data to support dask.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:100: extract_pandas_data (from tensorflow.contrib.learn.python.learn.learn_io.pandas_io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please access pandas data directly.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:159: DataFeeder.__init__ (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:340: check_array (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please convert numpy dtypes explicitly.\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:182: infer_real_valued_columns_from_input_fn (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please specify feature columns explicitly.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:660: regression_head (from tensorflow.contrib.learn.python.learn.estimators.head) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.contrib.estimator.*_head.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1179: BaseEstimator.__init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please replace uses of any Estimator from tf.contrib.learn with an Estimator from tf.estimator.*\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:427: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/0t/4zpm5shx0q9chl3vh_szs3280000gn/T/tmpdb0umk64\n",
      "WARNING:tensorflow:From <ipython-input-61-243d79292383>:15: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-61-243d79292383>:15: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-61-243d79292383>:15: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:508: SKCompat.__init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to the Estimator interface.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:98: extract_dask_labels (from tensorflow.contrib.learn.python.learn.learn_io.dask_io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please feed input to tf.data to support dask.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:102: extract_pandas_labels (from tensorflow.contrib.learn.python.learn.learn_io.pandas_io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please access pandas data directly.\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:678: ModelFnOps.__new__ (from tensorflow.contrib.learn.python.learn.estimators.model_fn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.EstimatorSpec. You can use the `estimator_spec` method to create an equivalent one.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNRegressor(params={'head': <tensorflow.contrib.learn.python.learn.estimators.head._RegressionHead object at 0x1c2d8c9908>, 'hidden_units': [50, 20], 'feature_columns': (_RealValuedColumn(column_name='', dimension=4, default_value=None, dtype=tf.float64, normalizer=None),), 'optimizer': <tensorflow.python.training.adam.AdamOptimizer object at 0x114a552b0>, 'activation_fn': <function leaky_relu at 0x1a2a211ea0>, 'dropout': None, 'gradient_clip_norm': None, 'embedding_lr_multipliers': None, 'input_layer_min_slice_size': None})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.contrib import learn\n",
    "import tensorflow as tf\n",
    "\n",
    "# Stacking multiple regressors using DNN\n",
    "features_blend = learn.infer_real_valued_columns_from_input(stack[['rf','gbr','ada','bagging']])\n",
    "\n",
    "# Optimizer algorithm\n",
    "adam = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "# Build multi-layer DNN for regression\n",
    "blend_nn = learn.DNNRegressor(feature_columns=features_blend, hidden_units=[50, 20], \n",
    "                              optimizer=adam, activation_fn=tf.nn.leaky_relu)\n",
    "#                               config=learn.estimators.RunConfig(num_cores=8))\n",
    "# Fit DNN\n",
    "blend_nn.fit(x=stack[['rf','gbr','ada','bagging']], y=stack['true'], steps=8000, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT: RF for Regression\n",
    "y_test_rf = estimator_rf.predict(X_test).flatten().tolist()\n",
    "\n",
    "# PREDICT: GBR \n",
    "y_test_gbr = estimator_gbr.predict(X_test).flatten().tolist()\n",
    "\n",
    "# PREDICT: ADA\n",
    "y_test_ada = estimator_ada.predict(X_test).flatten().tolist()\n",
    "\n",
    "# PREDICT: BAGGING\n",
    "y_test_bagging = estimator_bagging.predict(X_test).flatten().tolist()\n",
    "\n",
    "# PREDICT: KNN\n",
    "y_test_knn = estimator_knn.predict(X_test).flatten().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stack = pd.DataFrame(data={'rf':y_test_rf, 'gbr':y_test_gbr,\n",
    "                               'ada':y_test_ada,'bagging': y_test_bagging,\n",
    "                               'true':y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling DNNRegressor.predict (from tensorflow.contrib.learn.python.learn.estimators.dnn) with outputs=None is deprecated and will be removed after 2017-03-01.\n",
      "Instructions for updating:\n",
      "Please switch to predict_scores, or set `outputs` argument.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:779: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
     ]
    }
   ],
   "source": [
    "y_new_bl = list(blend_nn.predict(new_stack[['rf', 'gbr', 'ada','bagging']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "888024.0\n"
     ]
    }
   ],
   "source": [
    "model_predictions_ensemble = convertToPrediction(y_for_calculations,y_new_bl)\n",
    "print(profit(actual_predictions,model_predictions_ensemble).sum())\n",
    "print(\"cost is:\" + str(cost(model_predictions_ensemble,2).sum()) )\n",
    "print(\"profit is:\" + str(profit(actual_predictions,model_predictions_ensemble).sum()))\n",
    "print(\"profit percentage is:\" + str(profit_percentage(actual_predictions,model_predictions_ensemble, 3, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [1, 1, 1, 1]  # weights\n",
    "pred_agg = np.c_[pred_rf,pred_gbr,pred_ada,pred_bagging]\n",
    "pred_avr = np.average(pred_agg, axis=1, weights=w)\n",
    "model_predictions_avr = convertToPrediction(y_for_calculations,pred_avr)\n",
    "print(profit(actual_predictions,model_predictions_avr).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"RMSLE Value: \",rmsle(actual_predictions,model_predictions_avr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cost is:\" + str(cost(model_predictions_avr,2).sum()) )\n",
    "print(\"profit is:\" + str(profit(actual_predictions,model_predictions_avr).sum()))\n",
    "print(\"profit percentage is:\" + str(profit_percentage(actual_predictions,model_predictions_avr, 3, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
